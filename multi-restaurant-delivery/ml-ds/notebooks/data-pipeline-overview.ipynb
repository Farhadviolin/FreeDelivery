{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9dc18c9",
   "metadata": {},
   "source": [
    "# Advanced Data Pipelines & Real-Time Analytics\n",
    "\n",
    "Diese Übersicht zeigt die wichtigsten Bausteine für eine skalierbare, moderne Dateninfrastruktur mit Streaming, Batch, Lakehouse und Self-Service Analytics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31510f31",
   "metadata": {
    "vscode": {
     "languageId": "bash"
    }
   },
   "outputs": [],
   "source": [
    "# Kafka-Topics für Raw- und Aggregatdaten anlegen\n",
    "kafka-topics.sh --create --topic orders_raw --partitions 12 --replication-factor 3 --bootstrap-server kafka:9092\n",
    "kafka-topics.sh --create --topic orders_aggregates --partitions 6 --replication-factor 3 --bootstrap-server kafka:9092"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f9778c4",
   "metadata": {
    "vscode": {
     "languageId": "scala"
    }
   },
   "outputs": [],
   "source": [
    "// Flink Streaming-Job: Order Amount Aggregator\n",
    "import org.apache.flink.streaming.api.scala._\n",
    "import org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumer\n",
    "import org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducer\n",
    "import org.apache.flink.api.common.serialization._\n",
    "\n",
    "case class OrderEvent(orderId: String, userId: String, amount: Double, ts: Long)\n",
    "\n",
    "object OrderCountJob {\n",
    "  def main(args: Array[String]): Unit = {\n",
    "    val env = StreamExecutionEnvironment.getExecutionEnvironment\n",
    "    val props = // kafka properties\n",
    "    val consumer = new FlinkKafkaConsumer[OrderEvent](\"orders_raw\", new JSONDeserializationSchema[OrderEvent](), props)\n",
    "    val stream = env.addSource(consumer)\n",
    "      .keyBy(_.userId)\n",
    "      .window(TumblingEventTimeWindows.of(Time.minutes(1)))\n",
    "      .sum(\"amount\")\n",
    "    stream.addSink(new FlinkKafkaProducer[OrderEvent](\"orders_aggregates\", new JSONSerializationSchema[OrderEvent](), props))\n",
    "    env.execute(\"Order Amount Aggregator\")\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94157760",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Airflow DAG für Delta Lake Batch-Job\n",
    "from airflow import DAG\n",
    "from airflow.providers.apache.spark.operators.spark_submit import SparkSubmitOperator\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "default_args = {'start_date': datetime(2025,6,30), 'retries': 1, 'retry_delay': timedelta(minutes=10)}\n",
    "with DAG('delta_batch', schedule_interval='@daily', default_args=default_args, catchup=False) as dag:\n",
    "    run_spark = SparkSubmitOperator(\n",
    "        task_id='run_delta_job',\n",
    "        application='/opt/airflow/scripts/run_delta_job.py',\n",
    "        conf={'spark.master':'local[*]'},\n",
    "        deploy_mode='cluster'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50774e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delta Lake Spark Job: User-Aggregation\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"delta_batch\").getOrCreate()\n",
    "df = spark.read.format(\"delta\").load(\"s3://data/raw/orders/\")\n",
    "agg = df.groupBy(\"userId\").agg({\"amount\":\"sum\"}).withColumnRenamed(\"sum(amount)\",\"total_amount\")\n",
    "agg.write.format(\"delta\").mode(\"overwrite\").save(\"s3://data/processed/orders/\")\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99bf5fb5",
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "-- dbt Model: orders_summary\n",
    "with raw as (\n",
    "  select * from delta.`s3://data/processed/orders/`\n",
    ")\n",
    "select\n",
    "  userId,\n",
    "  total_amount,\n",
    "  rank() over (order by total_amount desc) as user_rank\n",
    "from raw"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e419f653",
   "metadata": {},
   "source": [
    "## Hinweise zu Serving, Monitoring & Data Quality\n",
    "\n",
    "- **Serving Layer:** Grafana visualisiert Echtzeitdaten aus Kafka/InfluxDB, Superset/Jupyter für Ad-hoc-Analysen auf Delta Lake via Trino.\n",
    "- **Monitoring:** Flink/Airflow/Batch-Jobs mit Prometheus/Grafana überwachen, Alerting bei Lags oder Fehlern.\n",
    "- **Data Quality:** Great Expectations-Checks in Airflow/Batch und dbt-Tests für alle Modelle integrieren.\n",
    "- **Self-Service:** dbt, Superset und JupyterHub für explorative Analysen bereitstellen."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
